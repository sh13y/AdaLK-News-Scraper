import os
import json
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from datetime import datetime

# Load environment variables
load_dotenv()

# Your news URL to scrape
NEWS_URL = "https://www.ada.lk/latest-news/11"

# File to store previously fetched news links
LOG_FILE = "news_log.json"
MARKDOWN_FILE = "README.md"

def fetch_full_content(news_url):
    response = requests.get(news_url)
    if response.status_code != 200:
        print(f"Failed to fetch content from {news_url}.")
        return ""

    soup = BeautifulSoup(response.text, "html.parser")
    content = soup.find("div", class_="single-body-wrap")

    if content:
        paragraphs = content.find_all("p")
        full_content = "\n\n".join([para.get_text(strip=True) for para in paragraphs])
        return full_content
    else:
        return "Full content not found."

def fetch_news():
    response = requests.get(NEWS_URL)
    if response.status_code != 200:
        print("Failed to fetch news.")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    news_items = []

    for news_div in soup.find_all("div", class_="row bg-white cat-b-row mt-3"):
        link = news_div.find("a", href=True)["href"]
        title = news_div.find("h5").get_text(strip=True)
        date = news_div.find("h6").get_text(strip=True).replace("â€¢", "").strip()
        short_desc = news_div.find("p", class_="cat-b-text").get_text(strip=True)
        
        # Handle missing images safely
        image_tag = news_div.find("img")
        image_url = image_tag["src"] if image_tag and "src" in image_tag.attrs else None

        # Fetch the full content for the news
        full_content = fetch_full_content(link)

        news_items.append({
            "link": link,
            "title": title,
            "date": date,
            "short_desc": short_desc,
            "image_url": image_url,
            "full_content": full_content
        })

    return news_items

def read_log():
    if not os.path.exists(LOG_FILE):
        return []
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def update_log(new_urls):
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            logged_data = json.load(f)
    else:
        logged_data = []

    logged_data.extend(new_urls)

    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(logged_data, f, indent=4)

def format_news_to_markdown(news_items):
    markdown_content = ""
    for item in news_items:
        try:
            news_date = datetime.strptime(item['date'], "%d %m %Y %H:%M:%S").strftime("%B %d, %Y, %I:%M %p")
        except ValueError:
            news_date = item['date']  # Use raw date if parsing fails

        markdown_content += f"\n\n---\n\n"
        markdown_content += f"## {item['title']}\n\n"
        markdown_content += f"\n*Published on: {news_date}*\n\n"
        # markdown_content += f"_{item['short_desc']}_\n\n"
        markdown_content += f"{item['full_content']}"

        # Only add the image if it exists
        if item['image_url']:
            markdown_content += f"\n\n![Image]({item['image_url']})\n\n"

    return markdown_content

def update_news_md(new_news):
    static_content = ""
    dynamic_content = ""

    # Read the existing README and separate static content
    if os.path.exists(MARKDOWN_FILE):
        with open(MARKDOWN_FILE, "r", encoding="utf-8") as f:
            content = f.read()
            if "<!-- STATIC-START -->" in content and "<!-- STATIC-END -->" in content:
                static_content = content.split("<!-- STATIC-END -->")[0] + "<!-- STATIC-END -->"
                dynamic_content = content.split("<!-- STATIC-END -->")[1].strip()

    # Generate markdown for new news
    new_news_markdown = format_news_to_markdown(new_news)

    # Combine static content, new news, and existing dynamic content
    updated_content = static_content + "\n\n" + new_news_markdown + "\n\n" + dynamic_content

    # Write back to README
    with open(MARKDOWN_FILE, "w", encoding="utf-8") as f:
        f.write(updated_content)

def main():
    news_items = fetch_news()
    logged_urls = read_log()
    new_news = [news for news in news_items if news['link'] not in logged_urls]

    if new_news:
        new_urls = [news['link'] for news in new_news]
        update_log(new_urls)
        update_news_md(new_news)
        print(f"Added {len(new_news)} new news articles to README.md.")
    else:
        print("No new news to add.")

if __name__ == "__main__":
    main()
